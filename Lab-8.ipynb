{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3USYGWxqdeJ",
        "outputId": "174f77b9-80e0-43fa-bfcc-dc853c4e931a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perform stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SSbfoPOqfBh",
        "outputId": "3671d311-bab4-4ddb-e07b-62a451a626d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/sohan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/sohan/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /Users/sohan/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed Words:\n",
            "['the', 'children', 'are', 'play', 'and', 'run', 'around', 'the', 'playground', '.']\n",
            "\n",
            "Lemmatized Words:\n",
            "['The', 'child', 'are', 'playing', 'and', 'running', 'around', 'the', 'playground', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')      # For tokenizing the text\n",
        "nltk.download('wordnet')    # For lemmatization\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "text = \"The children are playing and running around the playground.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stemming: Apply PorterStemmer to each token\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Lemmatization: Apply WordNetLemmatizer to each token\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Design a custom tokenizer and perform stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHZqmhuKqlnU",
        "outputId": "5f7d40d8-f6ec-4baa-e9d6-71a916e4b3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed Words:\n",
            "['check', 'out', 'my', 'new', 'blog', 'post']\n",
            "\n",
            "Lemmatized Words:\n",
            "['check', 'out', 'my', 'new', 'blog', 'post']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    # Remove URLs (http:// or https://)\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    # Tokenize the cleaned text using NLTK's word_tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove any remaining non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "# Example usage\n",
        "text = \"Check out my new blog post! #TechBlog @john_doe https://example.com\"\n",
        "tokens = custom_tokenizer(text)\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stemming: Apply PorterStemmer to each token\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Lemmatization: Apply WordNetLemmatizer to each token\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWCcQOT_reyh",
        "outputId": "c8943c56-a363-40e0-b7f8-6322eada026f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (5.2.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py\", line 13, in <module>\n",
            "    from . import pipeline  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/__init__.py\", line 1, in <module>\n",
            "    from .attributeruler import AttributeRuler\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/attributeruler.py\", line 8, in <module>\n",
            "    from ..language import Language\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/language.py\", line 43, in <module>\n",
            "    from .pipe_analysis import analyze_pipes, print_pipe_analysis, validate_attrs\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/pipe_analysis.py\", line 6, in <module>\n",
            "    from .tokens import Doc, Span, Token\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/__init__.py\", line 1, in <module>\n",
            "    from ._serialize import DocBin\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/_serialize.py\", line 14, in <module>\n",
            "    from ..vocab import Vocab\n",
            "  File \"spacy/vocab.pyx\", line 1, in init spacy.vocab\n",
            "  File \"spacy/tokens/doc.pyx\", line 49, in init spacy.tokens.doc\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/schemas.py\", line 195, in <module>\n",
            "    class TokenPatternString(BaseModel):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py\", line 286, in __new__\n",
            "    cls.__try_update_forward_refs__()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py\", line 808, in __try_update_forward_refs__\n",
            "    update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 554, in update_model_forward_refs\n",
            "    update_field_forward_refs(f, globalns=globalns, localns=localns)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 529, in update_field_forward_refs\n",
            "    update_field_forward_refs(sub_f, globalns=globalns, localns=localns)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 520, in update_field_forward_refs\n",
            "    field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 66, in evaluate_forwardref\n",
            "    return cast(Any, type_)._evaluate(globalns, localns, set())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NybRXh1X2PsZ",
        "outputId": "411549da-44ce-4472-d4fa-416e66eb1e47"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to normalize text (remove or standardize dates, monetary values, and numbers)\n",
        "def normalize_text(text):\n",
        "    # Normalize dates (example: replace any date with 'DATE')\n",
        "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', 'DATE', text)  # Matches formats like 12/05/2021\n",
        "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', 'DATE', text)  # Matches formats like 2021-12-05\n",
        "    text = re.sub(r'\\b\\d{1,2} \\w+ \\d{4}\\b', 'DATE', text)  # Matches formats like 12 May 2021\n",
        "\n",
        "    # Normalize monetary values (example: replace any monetary amount with 'MONEY')\n",
        "    text = re.sub(r'\\$\\d+(?:,\\d{3})*(?:\\.\\d{2})?', 'MONEY', text)  # Matches dollar values like $1,000.50\n",
        "    text = re.sub(r'\\b\\d+(?:,\\d{3})*(?:\\.\\d+)?\\s?(usd|euro|gbp|inr)\\b', 'MONEY', text, flags=re.IGNORECASE)  # Matches currency like 1000 USD\n",
        "\n",
        "    # Normalize numbers (example: replace any number with 'NUMBER')\n",
        "    text = re.sub(r'\\b\\d+\\b', 'NUMBER', text)  # Matches any number (e.g., 123, 12345)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function for Named Entity Recognition\n",
        "def named_entity_recognition(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract entities\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append((ent.text, ent.label_))  # Store the entity and its label\n",
        "\n",
        "    return entities\n",
        "\n",
        "text = \"Apple Inc. was founded on April 1, 1976 by Steve Jobs. The price of the iPhone is $999. \" \\\n",
        "        \"On 12/05/2023, the company announced a partnership with Microsoft. A person who earned 5000 USD \" \\\n",
        "        \"on 10th May 2023. Contact John at john@example.com or visit our office at 123 Park Ave.\"\n",
        "\n",
        "normalized_text = normalize_text(text)\n",
        "print(\"Normalized Text:\")\n",
        "print(normalized_text)\n",
        "\n",
        "entities = named_entity_recognition(text)\n",
        "print(\"\\nNamed Entities Recognized:\")\n",
        "for entity, label in entities:\n",
        "    print(f\"{entity}: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWKCI6Zd5Cd_"
      },
      "source": [
        "# 07/03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiAk171q28C6",
        "outputId": "dfa0124d-e455-4c57-af44-9d712cda9a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 1:\n",
            "Original: OMG!!! This is funny lol 😂\n",
            "Normalized: Oh my goodness! This is funny laughing out loud laughing with tears\n",
            "\n",
            "Test 2:\n",
            "Original: idk why you are so upset tbh... :)\n",
            "Normalized: I don't know why you are so upset tbh. Smiling\n",
            "\n",
            "Test 3:\n",
            "Original: btw, thx for the help 👍\n",
            "Normalized: By the way, thanks for the help thumbs up\n",
            "\n",
            "Test 4:\n",
            "Original: lol that's a good one ❤️\n",
            "Normalized: Laughing out loud that's a good one love\n",
            "\n",
            "Test 5:\n",
            "Original: omg!!! multiple punctuation marks?!?!\n",
            "Normalized: Oh my goodness! Multiple punctuation marks? ! ? !\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalizes text by replacing slangs, abbreviations, and emojis with formal counterparts,\n",
        "    and normalizing irregular spaces and punctuation.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to normalize\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Dictionary for common slangs and abbreviations (limited to 5)\n",
        "    slang_dict = {\n",
        "        \"lol\": \"laughing out loud\",\n",
        "        \"idk\": \"I don't know\",\n",
        "        \"btw\": \"by the way\",\n",
        "        \"omg\": \"oh my goodness\",\n",
        "        \"thx\": \"thanks\"\n",
        "    }\n",
        "\n",
        "    # Dictionary for emojis (limited to 5)\n",
        "    emoji_dict = {\n",
        "        \"😊\": \"smiling\",\n",
        "        \"😂\": \"laughing with tears\",\n",
        "        \"👍\": \"thumbs up\",\n",
        "        \"❤️\": \"love\",\n",
        "        \":)\": \"smiling\"\n",
        "    }\n",
        "\n",
        "    # Convert text to lowercase for easier matching\n",
        "    normalized_text = text.lower()\n",
        "\n",
        "    # Replace slangs and abbreviations\n",
        "    # Add word boundaries to avoid replacing parts of words\n",
        "    for slang, formal in slang_dict.items():\n",
        "        normalized_text = re.sub(r'\\b' + slang + r'\\b', formal, normalized_text)\n",
        "\n",
        "    # Replace emojis\n",
        "    for emoji, description in emoji_dict.items():\n",
        "        normalized_text = normalized_text.replace(emoji, f\" {description} \")\n",
        "\n",
        "    # Normalize spaces (replace multiple spaces with a single space)\n",
        "    normalized_text = re.sub(r'\\s+', ' ', normalized_text)\n",
        "\n",
        "    # Normalize punctuation (replace multiple occurrences with a single one)\n",
        "    for punct in '.!?,:;-':\n",
        "        normalized_text = re.sub(r'[' + re.escape(punct) + r']+', punct, normalized_text)\n",
        "\n",
        "    # Ensure proper spacing after punctuation\n",
        "    for punct in '.!?,:;':\n",
        "        normalized_text = re.sub(r'[' + re.escape(punct) + r']', punct + ' ', normalized_text)\n",
        "    normalized_text = re.sub(r'\\s+', ' ', normalized_text)  # Fix any double spaces created\n",
        "\n",
        "    # Capitalize the first letter of each sentence\n",
        "    normalized_text = re.sub(r'(^|[.!?]\\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), normalized_text)\n",
        "\n",
        "    # Remove leading/trailing whitespace\n",
        "    normalized_text = normalized_text.strip()\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "# Test the function with various inputs\n",
        "def test_normalize_text():\n",
        "    test_cases = [\n",
        "        \"OMG!!! This is funny lol 😂\",\n",
        "        \"idk why you are so upset tbh... :)\",\n",
        "        \"btw, thx for the help 👍\",\n",
        "        \"lol that's a good one ❤️\",\n",
        "        \"omg!!! multiple punctuation marks?!?!\"\n",
        "    ]\n",
        "\n",
        "    for i, test in enumerate(test_cases):\n",
        "        print(f\"Test {i+1}:\")\n",
        "        print(f\"Original: {test}\")\n",
        "        print(f\"Normalized: {normalize_text(test)}\")\n",
        "        print()\n",
        "\n",
        "# Run the tests\n",
        "if __name__ == \"__main__\":\n",
        "    test_normalize_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQkrjpB05GQk",
        "outputId": "2cf97aca-4fc1-493d-f068-24e18c2c486c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Normalization Results:\n",
            "--------------------------\n",
            "Example 1:\n",
            "Original: OMG!!! Have u SEEN this???\n",
            "Normalized: oh my goodness! have you seen this?\n",
            "\n",
            "Example 2:\n",
            "Original: LOL that's SO funny btw, IDK y ppl r mad!!!!\n",
            "Normalized: laughing out loud that's so funny by the way, i don't know why ppl are mad!\n",
            "\n",
            "Example 3:\n",
            "Original: ThX for Ur help..... K???\n",
            "Normalized: thanks for your help. okay?\n",
            "\n",
            "Example 4:\n",
            "Original: Y r U so LATE?? BTW, the movie starts at 8pm...\n",
            "Normalized: why are you so late? by the way, the movie starts at 8pm.\n",
            "\n",
            "Example 5:\n",
            "Original: This is a Normal Sentence, But with Weird Capitalization.\n",
            "Normalized: this is a normal sentence, but with weird capitalization.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalizes text by:\n",
        "    1. Replacing slang words with their full forms\n",
        "    2. Standardizing punctuation\n",
        "    3. Converting the text to lowercase\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to normalize\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Convert to lowercase first\n",
        "    text = text.lower()\n",
        "\n",
        "    # Dictionary of slang words and their full forms\n",
        "    slang_dict = {\n",
        "        \"lol\": \"laughing out loud\",\n",
        "        \"idk\": \"i don't know\",\n",
        "        \"btw\": \"by the way\",\n",
        "        \"omg\": \"oh my goodness\",\n",
        "        \"thx\": \"thanks\",\n",
        "        \"u\": \"you\",\n",
        "        \"r\": \"are\",\n",
        "        \"ur\": \"your\",\n",
        "        \"y\": \"why\",\n",
        "        \"k\": \"okay\"\n",
        "    }\n",
        "\n",
        "    # Replace slang words\n",
        "    for slang, full_form in slang_dict.items():\n",
        "        # Use word boundaries to avoid replacing parts of words\n",
        "        text = re.sub(r'\\b' + slang + r'\\b', full_form, text)\n",
        "\n",
        "    # Standardize punctuation (replace multiple occurrences with a single one)\n",
        "    for punct in '.!?,:;-':\n",
        "        text = re.sub(r'[' + re.escape(punct) + r']+', punct, text)\n",
        "\n",
        "    # Add proper spacing after punctuation\n",
        "    for punct in '.!?,:;':\n",
        "        text = re.sub(r'([' + re.escape(punct) + r'])([^\\s])', r'\\1 \\2', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test the function\n",
        "def main():\n",
        "    test_sentences = [\n",
        "        \"OMG!!! Have u SEEN this???\",\n",
        "        \"LOL that's SO funny btw, IDK y ppl r mad!!!!\",\n",
        "        \"ThX for Ur help..... K???\",\n",
        "        \"Y r U so LATE?? BTW, the movie starts at 8pm...\",\n",
        "        \"This is a Normal Sentence, But with Weird Capitalization.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Text Normalization Results:\")\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences, 1):\n",
        "        normalized = normalize_text(sentence)\n",
        "        print(f\"Example {i}:\")\n",
        "        print(f\"Original: {sentence}\")\n",
        "        print(f\"Normalized: {normalized}\")\n",
        "        print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVaGldyJ5hwJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
